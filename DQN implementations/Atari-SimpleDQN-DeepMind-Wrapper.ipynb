{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9b593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/deonix/maturarbeit-code/DQN/SimpleDQNtest.ipynb\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 15:52:33.552509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:33.558201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:33.558434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:33.559362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-12 15:52:33.559702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:33.559895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:33.560053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:34.005919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:34.006119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:34.006275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-12 15:52:34.006392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4914 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:1f:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from DQN.SimpleDQNtest import Agent\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import losses\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5472f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 84, 84, 4)]       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 16)        4112      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 9, 9, 32)          8224      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2592)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               663808    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 677,172\n",
      "Trainable params: 677,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model file (in ./models):BreakoutNoFrameskip-v4-DQN-ERROR-12-06-2022-13-02\n",
      "replay memory file (enter x if none): x\n",
      "LOADED ./models/BreakoutNoFrameskip-v4-DQN-ERROR-12-06-2022-13-02.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 15:53:40.121262: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-06-12 15:53:40.399967: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-12 15:53:40.400521: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-12 15:53:40.400539: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-06-12 15:53:40.400870: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-12 15:53:40.400959: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:          0\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          1\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          2\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          3\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          4\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          5\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          6\n",
      "Score:          2.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          7\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          8\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:          9\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         10\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         11\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         12\n",
      "Score:          1.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         13\n",
      "Score:          1.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         14\n",
      "Score:          1.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         15\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         16\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         17\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         18\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         19\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         20\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         21\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         22\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         23\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         24\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         25\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         26\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         27\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         28\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         29\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         30\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         31\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         32\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         33\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         34\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         35\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         36\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         37\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         38\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         39\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         40\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         41\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         42\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         43\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         44\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         45\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         46\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         47\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n",
      "Episode:         48\n",
      "Score:          0.0\n",
      "Epsilon: 0.1\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "#agent and environment optimized for Atari games\n",
    "#https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "class AtariGame:\n",
    "    def __init__(self, env_name, render_mode=None, name=None):\n",
    "        self.name = name\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes          = 5e6\n",
    "        self.env_name          = env_name\n",
    "        self.env               = wrap_deepmind(env = make_atari(self.env_name), frame_stack=True, scale=True)\n",
    "        self.frame_skip        = 4 #skip every 4th frame\n",
    "        #84x84 greyscale\n",
    "        self.reduzed_size      = (84, 84, self.frame_skip) #, greyscale(1), 84x84, 4 Pictures, \n",
    "        self.state_size        = self.reduzed_size\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        self.termination_index = 10000\n",
    "        self.history           = []\n",
    "        self.save_freq         = 10000\n",
    "        self.agent             = Agent(self.state_size, self.action_size, \n",
    "                                       #Parameters taken from Deepmind Breakout AI\n",
    "                                       #input model layers as keras.layers objects\n",
    "                                       anatomy=[layers.Conv2D(16,8,strides=4,activation=activations.relu),\n",
    "                                                layers.Conv2D(32,4,strides=2,activation=activations.relu),\n",
    "                                                layers.Flatten(),\n",
    "                                                layers.Dense(256, activation=activations.relu)],\n",
    "                                       name=f\"{self.env_name}-DQN\",\n",
    "                                       linear_decrease=True,\n",
    "                                       epsilon=1,\n",
    "                                       epsilon_decay=0.9/31000,\n",
    "                                       epsilon_min=0.1,\n",
    "                                       model_verbose=0,\n",
    "                                       lr = 0.0025,\n",
    "                                       gamma= 0.99,\n",
    "                                       loss = losses.Huber(),\n",
    "                                       max_memory_size=0.7e5\n",
    "                                       )\n",
    "        #print(self.state_size, self.action_size)\n",
    "    \n",
    "    def run(self, load_model = False, skip_training=False, overwrite_epsilon=-1, save=True, logs=False, log_freq=1):\n",
    "        #LOGS ARE ALWAYS ENABLED\n",
    "        logging.basicConfig(filename=f\"models/{self.env_name}-DQN.log\",\n",
    "                level=logging.INFO,\n",
    "                format='%(levelname)s: %(asctime)s %(message)s',\n",
    "                datefmt='%d/%m/%Y %I:%M:%S')\n",
    "        \n",
    "        #if true, try to load existing model\n",
    "        if load_model:\n",
    "            self.agent.load_model(overwrite_epsilon=overwrite_epsilon)\n",
    "        try:\n",
    "            training_batches = 0\n",
    "            for index_episode in range(int(self.episodes)):\n",
    "                state = tf.expand_dims(np.array(self.env.reset()), 0)\n",
    "                done = False\n",
    "                score = 0 \n",
    "                q = 0\n",
    "                q_n = 0\n",
    "                \n",
    "                for index in range(0, self.termination_index):\n",
    "                    if index > self.frame_skip:\n",
    "                        action = self.agent.pick_action(state) \n",
    "                    else:\n",
    "                        action = np.random.choice(self.agent.action_size)\n",
    "                    \n",
    "                    \n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    score += reward\n",
    "                    next_state = tf.expand_dims(np.array(next_state), 0)\n",
    "                    #print(next_state.numpy().shape)\n",
    "                    \n",
    "                    self.agent.update_memory((state.numpy(), action, reward, \n",
    "                                                      next_state.numpy(), done))\n",
    "                    \n",
    "                    if (index % self.frame_skip == 0) and not skip_training:\n",
    "                        if index > self.frame_skip:\n",
    "                            q += np.amax(self.agent.predict(state))\n",
    "                            q_n += 1\n",
    "\n",
    "                            training_batches += 1\n",
    "                            self.agent.replay()                     \n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                self.history.append(score)\n",
    "                if len(self.history) > 100:\n",
    "                    del self.history[:1]\n",
    "                \n",
    "                print(f\"Episode: {index_episode:-10}\")\n",
    "                print(f\"Score: {score:-12}\")\n",
    "                print(f\"Epsilon: {self.agent.exploration_rate}\")\n",
    "                print(\"\".join([\"_\" for i in range(10)]))\n",
    "                if logs and index_episode % log_freq == 0:\n",
    "                    running_reward = sum(self.history) / len(self.history)\n",
    "                    logging.info(f\"EPISODE: {index_episode}\")\n",
    "                    logging.info(f\"AVG LAST {len(self.history)} REWARDS: {running_reward:0.2f}\")\n",
    "                    logging.info(f\"BATCHES TRAINED: {training_batches}\")\n",
    "                    logging.info(f\"AVG Q VALUE: {(q / q_n):.5f}\")\n",
    "                    logging.info(f\"SCORE: {score}\")\n",
    "                    logging.info(f\"DURATION (STEPS): {index}\")\n",
    "                    logging.info(f\"EPSILON: {self.agent.exploration_rate:.5f}\")\n",
    "                    logging.info(f\"MEMORY SIZE: {len(self.agent.memory)}\")\n",
    "                    logging.info(\"\".join([\"-\" for i in range(12)]))\n",
    "                \n",
    "                if index_episode % self.save_freq == 0 and save and index_episode != 0:\n",
    "                    self.agent.save_model(name=\"EP\"+str(index_episode/1000)+\"k\", save_memory=False)\n",
    "                \n",
    "        except:\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"ERROR\", save_memory=True)\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            logging.exception(\"An error has occured\")\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            \n",
    "        finally:\n",
    "            #save model upon interrupting\n",
    "            if logs or save:\n",
    "                logging.info(f\"TRAINING FINISHED AFTER {training_batches} BATCHES\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "                if save:\n",
    "                    model_name = self.agent.build_name(name=\"FINISHED\")\n",
    "                    logging.info(f\"MODEL NAME: {model_name}\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            #print(training_batches)\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"FINISHED\", save_memory=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #render modes: \"human\", None\n",
    "    atari = AtariGame(\"BreakoutNoFrameskip-v4\", render_mode=None)\n",
    "    atari.run(load_model=True, skip_training=False, overwrite_epsilon=-1, save=True, logs=True, log_freq=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffceac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ce28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
