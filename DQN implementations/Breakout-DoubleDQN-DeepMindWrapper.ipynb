{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9b593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/deonix/maturarbeit-code/DQN/DoubleDQN.ipynb\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 16:46:55.243722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.249128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.249345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.250235: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-27 16:46:55.250596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.250795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.250956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.682067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.682253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.682401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 16:46:55.682515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5202 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:1f:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from DQN.DoubleDQN import Agent\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from tensorflow.keras import optimizers\n",
    "import lz4\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5472f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 84, 84, 4)]       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "++++++++++++\n",
      "1\n",
      "[-0.06983794 -0.0923739  -0.10894585 -0.06592628]\n",
      "tf.Tensor(-0.06575054, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.06509303, shape=(), dtype=float32)\n",
      "tf.Tensor([-0.06983794 -0.06509303 -0.10894585 -0.06592628], shape=(4,), dtype=float32)\n",
      "++++\n",
      "1\n",
      "[-0.07022618 -0.09315348 -0.10828301 -0.06687327]\n",
      "tf.Tensor(-0.06531768, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.064664505, shape=(), dtype=float32)\n",
      "tf.Tensor([-0.07022618 -0.06466451 -0.10828301 -0.06687327], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "2\n",
      "[-0.04853726  0.05070566 -0.00586978 -0.06543225]\n",
      "tf.Tensor(0.050954007, shape=(), dtype=float32)\n",
      "tf.Tensor(0.05044447, shape=(), dtype=float32)\n",
      "tf.Tensor([-0.04853726  0.05070566  0.05044447 -0.06543225], shape=(4,), dtype=float32)\n",
      "++++\n",
      "3\n",
      "[-0.0457358   0.0496772  -0.00601173 -0.06902566]\n",
      "tf.Tensor(0.051066246, shape=(), dtype=float32)\n",
      "tf.Tensor(0.050555583, shape=(), dtype=float32)\n",
      "tf.Tensor([-0.0457358   0.0496772  -0.00601173  0.05055558], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "1\n",
      "[0.03728142 0.07313679 0.06531733 0.00226718]\n",
      "tf.Tensor(0.07294152, shape=(), dtype=float32)\n",
      "tf.Tensor(0.07221211, shape=(), dtype=float32)\n",
      "tf.Tensor([0.03728142 0.07221211 0.06531733 0.00226718], shape=(4,), dtype=float32)\n",
      "++++\n",
      "2\n",
      "[ 0.04338753  0.07076494  0.07006332 -0.00074709]\n",
      "tf.Tensor(0.07181451, shape=(), dtype=float32)\n",
      "tf.Tensor(0.07109636, shape=(), dtype=float32)\n",
      "tf.Tensor([ 0.04338753  0.07076494  0.07109636 -0.00074709], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "3\n",
      "[0.10099959 0.07945614 0.14530429 0.07484356]\n",
      "tf.Tensor(0.14683342, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14536509, shape=(), dtype=float32)\n",
      "tf.Tensor([0.10099959 0.07945614 0.14530429 0.14536509], shape=(4,), dtype=float32)\n",
      "++++\n",
      "2\n",
      "[0.09757165 0.08117884 0.14523588 0.07748525]\n",
      "tf.Tensor(0.14382958, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1423913, shape=(), dtype=float32)\n",
      "tf.Tensor([0.09757165 0.08117884 0.1423913  0.07748525], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "0\n",
      "[0.1559408  0.08577886 0.23064426 0.16290763]\n",
      "tf.Tensor(0.23062158, shape=(), dtype=float32)\n",
      "tf.Tensor(0.22831537, shape=(), dtype=float32)\n",
      "tf.Tensor([0.22831537 0.08577886 0.23064426 0.16290763], shape=(4,), dtype=float32)\n",
      "++++\n",
      "3\n",
      "[0.16276282 0.08358297 0.23341103 0.16347939]\n",
      "tf.Tensor(0.23619996, shape=(), dtype=float32)\n",
      "tf.Tensor(0.23383796, shape=(), dtype=float32)\n",
      "tf.Tensor([0.16276282 0.08358297 0.23341103 0.23383796], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "3\n",
      "[0.24720573 0.12848288 0.30899894 0.2588294 ]\n",
      "tf.Tensor(0.30867532, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30558857, shape=(), dtype=float32)\n",
      "tf.Tensor([0.24720573 0.12848288 0.30899894 0.30558857], shape=(4,), dtype=float32)\n",
      "++++\n",
      "0\n",
      "[0.24585794 0.13039935 0.30693316 0.25664145]\n",
      "tf.Tensor(0.3075874, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30451152, shape=(), dtype=float32)\n",
      "tf.Tensor([0.30451152 0.13039935 0.30693316 0.25664145], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "1\n",
      "[0.35056126 0.19261007 0.4126007  0.35578865]\n",
      "tf.Tensor(0.41272435, shape=(), dtype=float32)\n",
      "tf.Tensor(0.4085971, shape=(), dtype=float32)\n",
      "tf.Tensor([0.35056126 0.4085971  0.4126007  0.35578865], shape=(4,), dtype=float32)\n",
      "++++\n",
      "3\n",
      "[0.35527468 0.1924881  0.41796955 0.35682687]\n",
      "tf.Tensor(0.41252285, shape=(), dtype=float32)\n",
      "tf.Tensor(0.40839761, shape=(), dtype=float32)\n",
      "tf.Tensor([0.35527468 0.1924881  0.41796955 0.40839761], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "2\n",
      "[0.4773678  0.31437007 0.515553   0.47315282]\n",
      "tf.Tensor(0.51638526, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5112214, shape=(), dtype=float32)\n",
      "tf.Tensor([0.4773678  0.31437007 0.5112214  0.47315282], shape=(4,), dtype=float32)\n",
      "++++\n",
      "2\n",
      "[0.47956488 0.31217703 0.5173948  0.4753895 ]\n",
      "tf.Tensor(0.5163055, shape=(), dtype=float32)\n",
      "tf.Tensor(0.51114243, shape=(), dtype=float32)\n",
      "tf.Tensor([0.47956488 0.31217703 0.51114243 0.4753895 ], shape=(4,), dtype=float32)\n",
      "++++++++++++\n",
      "3\n",
      "[0.6032478 0.5053293 0.6204375 0.6076302]\n",
      "tf.Tensor(0.6234723, shape=(), dtype=float32)\n",
      "tf.Tensor(0.61723757, shape=(), dtype=float32)\n",
      "tf.Tensor([0.6032478  0.5053293  0.6204375  0.61723757], shape=(4,), dtype=float32)\n",
      "++++\n",
      "3\n",
      "[0.60008603 0.50686675 0.6176463  0.6042082 ]\n",
      "tf.Tensor(0.6200419, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6138415, shape=(), dtype=float32)\n",
      "tf.Tensor([0.60008603 0.50686675 0.6176463  0.6138415 ], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#agent and environment optimized for Atari games\n",
    "#https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "class AtariGame:\n",
    "    def __init__(self, env_name, render_mode=None, name=None):\n",
    "        self.name = name\n",
    "        self.episodes          = 5e9\n",
    "        self.env_name          = env_name\n",
    "        self.render_mode       = render_mode\n",
    "        self.env               = wrap_deepmind(env = make_atari(self.env_name, render_mode=render_mode),\n",
    "                                               frame_stack=True, scale=True)\n",
    "        self.frame_skip        = 4\n",
    "        self.replay_rate       = 4\n",
    "        #84x84 greyscale\n",
    "        #self.reduzed_size      = (84, 84, self.frame_skip) #, greyscale(1), 84x84, 4 Pictures, \n",
    "        self.state_size        = self.env.observation_space.shape\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        self.termination_index = 1000\n",
    "        self.env._max_episode_steps = self.termination_index\n",
    "        self.history           = []\n",
    "        self.save_freq         = 10000\n",
    "        self.update_freq       = 10000\n",
    "        self.target_score      = 500\n",
    "        self.enable_target_score = False,\n",
    "        self.random_frames     = 50000\n",
    "        self.agent             = Agent(self.state_size, self.action_size,\n",
    "                                     name=f\"{self.env_name}-DoubleDQN-run2\",\n",
    "                                     anatomy=[layers.Conv2D(32, 8, strides=4, activation=\"relu\"),\n",
    "                                              layers.Conv2D(64, 4, strides=2, activation=\"relu\"),\n",
    "                                              layers.Conv2D(64, 3, strides=1, activation=\"relu\"),\n",
    "                                              layers.Flatten(),\n",
    "                                              layers.Dense(512, activation=\"relu\")],\n",
    "                                     lr=0.00025,\n",
    "                                     epsilon_min=0.1,\n",
    "                                     linear_decrease=True,\n",
    "                                     epsilon_decay=0.9/32000,\n",
    "                                     disable_double=True,\n",
    "                                     max_memory_size=1e5,\n",
    "                                     gamma=0.99,\n",
    "                                     optimizer=optimizers.Adam(learning_rate=0.00025, clipnorm=1.0),\n",
    "                                     loss_function=losses.Huber())\n",
    "                        \n",
    "        #print(self.state_size, self.action_size)\n",
    "    \n",
    "    def run(self, load_model = False, skip_training=False, overwrite_epsilon=-1, save=True, logs=False, log_freq=1):\n",
    "        #LOGS ARE ALWAYS ENABLED\n",
    "        logging.basicConfig(filename=f\"models/{self.agent.default_name}.log\",\n",
    "                level=logging.INFO,\n",
    "                format='%(levelname)s: %(asctime)s %(message)s',\n",
    "                datefmt='%d/%m/%Y %I:%M:%S')\n",
    "        \n",
    "        #if true, try to load existing model\n",
    "        if load_model:\n",
    "            self.agent.load_model(overwrite_epsilon=overwrite_epsilon)\n",
    "            \n",
    "        try:\n",
    "            training_batches = 0\n",
    "            frames = 0\n",
    "            for index_episode in range(int(self.episodes)):\n",
    "                state = self.env.reset()\n",
    "                state = np.array(state)\n",
    "                state_tensor = tf.expand_dims(state, 0)\n",
    "                \n",
    "                #state = np.array(state)\n",
    "                score = 0 \n",
    "                q = 0\n",
    "                q_n = 0\n",
    "    \n",
    "                for index in range(0, self.termination_index):\n",
    "            \n",
    "                    if frames >= self.random_frames:\n",
    "                        action = self.agent.pick_action(state_tensor)\n",
    "                    else:\n",
    "                        action = np.random.choice(self.action_size)\n",
    "                        \n",
    "                        \n",
    "                    q += tf.reduce_max(self.agent.predict(state_tensor))\n",
    "                    q_n += 1\n",
    "                    \n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    next_state = np.array(next_state)\n",
    "                    state_tensor = tf.expand_dims(state, 0)\n",
    "                    \n",
    "                    score += reward\n",
    "                    \n",
    "                    self.agent.update_memory(state=state, \n",
    "                                            reward=reward,\n",
    "                                            action=action,\n",
    "                                            state_next=next_state,\n",
    "                                            done=done)\n",
    "                    \n",
    "                    if (frames % self.replay_rate) == 0:\n",
    "                        training_batches += 1\n",
    "                        self.agent.replay(debug=False)\n",
    "                    \n",
    "                    if (frames % self.update_freq) == 0:\n",
    "                        self.agent.update_target()\n",
    "                        logging.info(f\"MODEL UPDATE\")\n",
    "                        \n",
    "                    state = next_state\n",
    "                    frames += 1\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                self.history.append(score)\n",
    "                if len(self.history) > 100:\n",
    "                    del self.history[:1]\n",
    "                \n",
    "                print(f\"Episode: {index_episode:-10}\")\n",
    "                print(f\"Score: {score:-12}\")\n",
    "                print(f\"Epsilon: {self.agent.exploration_rate}\")\n",
    "                print(\"\".join([\"_\" for i in range(10)]))\n",
    "                if logs and index_episode % log_freq == 0:\n",
    "                    running_reward = sum(self.history) / len(self.history)\n",
    "                    logging.info(f\"EPISODE: {index_episode}\")\n",
    "                    logging.info(f\"AVG LAST {len(self.history)} REWARDS: {running_reward:0.2f}\")\n",
    "                    logging.info(f\"BATCHES TRAINED: {training_batches}\")\n",
    "                    if q_n != 0:\n",
    "                        logging.info(f\"AVG Q VALUE: {(q / q_n):.5f}\")\n",
    "                    logging.info(f\"SCORE: {score}\")\n",
    "                    logging.info(f\"DURATION (STEPS): {index}\")\n",
    "                    logging.info(f\"EPSILON: {self.agent.exploration_rate:.5f}\")\n",
    "                    logging.info(f\"MEMORY SIZE: {len(self.agent.state_hist)}\")\n",
    "                    logging.info(\"\".join([\"-\" for i in range(12)]))\n",
    "                \n",
    "                if index_episode % self.save_freq == 0 and save and index_episode != 0:\n",
    "                    self.agent.save_model(name=\"EP\"+str(index_episode/1000)+\"k\", save_memory=False)\n",
    "                \n",
    "                if score > self.target_score and self.enable_target_score:\n",
    "                    self.agent.save_model(f\"TARGET-{self.target_score}\")\n",
    "                    break\n",
    "                    \n",
    "        except:\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"ERROR\", save_memory=False)\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            logging.exception(\"An error has occured\")\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            \n",
    "        finally:\n",
    "            #save model upon interrupting\n",
    "            if logs or save:\n",
    "                logging.info(f\"TRAINING FINISHED AFTER {training_batches} BATCHES\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "                if save:\n",
    "                    model_name = self.agent.build_name(name=\"FINISHED\")\n",
    "                    logging.info(f\"MODEL NAME: {model_name}\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            #print(training_batches)\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"FINISHED\", save_memory=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #render modes: \"human\", None\n",
    "    atari = AtariGame(\"BreakoutNoFrameskip-v4\", render_mode=None)\n",
    "    atari.run(load_model=False, skip_training=False, overwrite_epsilon=1, save=False, logs=True, log_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2da1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = wrap_deepmind(env = make_atari(\"BreakoutNoFrameskip-v4\", render_mode=None),\n",
    "#                                                frame_stack=True, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ea2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
