{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aef52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#for AMD gpus\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the agent\n",
    "class Agent: \n",
    "    def __init__(self, state_size, action_size,\n",
    "                #anatomy of the hidden layers, must add the object from tf.keras.layers directly\n",
    "                anatomy=[layers.Dense(24), layers.Dense(24)],\n",
    "                name=\"DeepQNetwork\",\n",
    "                lr=0.001,\n",
    "                #discount factor 0 <= gamma <= 1\n",
    "                gamma=0.95,\n",
    "                #probability that it takes an exploritory action\n",
    "                linear_decrease=False,\n",
    "                epsilon=1,\n",
    "                epsilon_min=0.0001,\n",
    "                epsilon_decay=0.995,\n",
    "                batch_size=32,\n",
    "                model_summary=True,\n",
    "                model_verbose=0,\n",
    "                max_memory_size=1e6,\n",
    "                loss=\"mse\",\n",
    "                optimizer=None\n",
    "                ):\n",
    "        \n",
    "        #model parameters\n",
    "        self.model_anatomy = anatomy \n",
    "        self.default_name = name\n",
    "        #self.weight_backup_default = \"cartpole_weight.h5\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.max_memory_size = max_memory_size\n",
    "        self.loss = loss\n",
    "        if optimizer != None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = Adam(learning_rate=lr, clipnorm=1.0)\n",
    "        #initiate memory\n",
    "        self.action_hist = []\n",
    "        self.state_hist = []\n",
    "        self.state_next_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.done_hist = []\n",
    "        \n",
    "        \n",
    "        #hyperparameters\n",
    "        self.learning_rate = lr\n",
    "        self.gamma = gamma\n",
    "        self.linear_decrease = linear_decrease\n",
    "        self.exploration_rate = epsilon\n",
    "        self.exploration_min = epsilon_min\n",
    "        self.exploration_decay = epsilon_decay\n",
    "        self.sample_batch_size = 32\n",
    "        \n",
    "        #debug options\n",
    "        self.model_summary = model_summary\n",
    "        self.model_verbose = model_verbose\n",
    "        #create model\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "    \n",
    " \n",
    "    def _build_model(self):\n",
    "        #input layer\n",
    "        inputs = keras.Input(shape=self.state_size)\n",
    "        \n",
    "        #create hidden layers\n",
    "        x = self.model_anatomy[0](inputs)\n",
    "        for layer in self.model_anatomy[1:]:\n",
    "            x = layer(x)\n",
    "        \"\"\"\n",
    "        #deprecated code for hidden layer generation\n",
    "        x = self.model_anatomy[0][0](**self.model_anatomy[0][1])(inputs)\n",
    "        for num_neurons in self.model_anatomy[1:]:\n",
    "            x = layers.Dense(num_neurons, activation=\"relu\")(x)\n",
    "        \"\"\"\n",
    "        #output layer\n",
    "        outputs = layers.Dense(self.action_size, activation = \"linear\")(x)\n",
    "        \n",
    "        #create model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=self.optimizer,\n",
    "                     loss=self.loss)\n",
    "        #model summary if enabled\n",
    "        if self.model_summary: model.summary()\n",
    "        return model\n",
    "    \n",
    "    def update_memory(self, state, reward, action, state_next, done):\n",
    "        #Format of step: [state(t), reward(t+1), action(t), state(t+1), done?]\n",
    "        \n",
    "        self.state_hist.append(state)\n",
    "        self.reward_hist.append(reward)\n",
    "        self.action_hist.append(action)\n",
    "        self.state_next_hist.append(state_next)\n",
    "        self.done_hist.append(done)\n",
    "        \n",
    "        if len(self.state_hist) > self.max_memory_size:\n",
    "            del self.state_hist[:1]\n",
    "            del self.reward_hist[:1]\n",
    "            del self.action_hist[:1]\n",
    "            del self.state_next_hist[:1]\n",
    "            del self.done_hist[:1]\n",
    "            \n",
    "    def pick_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            #return random move  \n",
    "            return np.random.choice(self.action_size)\n",
    "        \n",
    "        q_values = self.predict(tf.convert_to_tensor(state))[0]\n",
    "        \n",
    "        #return action with the highest expected reward\n",
    "        return_val = tf.argmax(q_values)\n",
    "        return return_val\n",
    "    \n",
    "    def predict(self, state, main=True):\n",
    "        #print(state.shape)\n",
    "        if main:\n",
    "            return self.model(state)\n",
    "        else:\n",
    "            return self.target_model(state)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.state_hist) < self.sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        #replay\n",
    "        #samples random experiences from memory\n",
    "        indices = np.random.choice(range(len(self.done_hist)), size=self.sample_batch_size)\n",
    "        \n",
    "        \n",
    "        state_batch = np.array([self.state_hist[i] for i in indices])\n",
    "        next_state_batch = np.array([self.state_next_hist[i] for i in indices])\n",
    "        action_batch = [self.action_hist[i] for i in indices]\n",
    "        reward_batch = [self.reward_hist[i] for i in indices]\n",
    "        done_batch = tf.convert_to_tensor([float(self.done_hist[i]) for i in indices])\n",
    "\n",
    "        #reshape into (batch_size, (frame size), frames from frame_skip)\n",
    "        #state_batch = tf.reshape(np.array(state_batch), (self.sample_batch_size, *self.state_size))\n",
    "        #next_state_batch = tf.reshape(np.array(next_state_batch), (self.sample_batch_size, *self.state_size))\n",
    "        \n",
    "        #use target for stability\n",
    "        fut_rewards = self.predict(next_state_batch, main=False)\n",
    "        \n",
    "        #update q values for all actions in s\n",
    "        updated_q_vals = reward_batch + self.gamma * tf.reduce_max(fut_rewards, axis=1)\n",
    "        updated_q_vals = updated_q_vals * (1 - done_batch) - done_batch\n",
    "        \n",
    "        masks = tf.one_hot(action_batch, self.action_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = self.predict(state_batch)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = self.loss(updated_q_vals, q_action)\n",
    "        \n",
    "                grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                        \n",
    "        #update exploration rate\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            if not self.linear_decrease:\n",
    "                self.exploration_rate *= self.exploration_decay\n",
    "            else:\n",
    "                self.exploration_rate -= self.exploration_decay\n",
    "    \n",
    "    def build_name(self, name=\"\"):\n",
    "        now = datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        return f\"{self.default_name}-{name}-{now}\"\n",
    "        \n",
    "    def save_model(self, name=\"\", save_memory=False):\n",
    "        self.model.save(f\"./models/{self.build_name(name=name)}.h5\")\n",
    "        if save_memory:\n",
    "            pass\n",
    "            #pickle.dump(self.memory, open(f\"./models/{self.build_name(name=name)}.pkl\", \"wb\"))\n",
    "        print(f\"MODEL SAVED AS ./models/{self.build_name(name=name)}.h5\")\n",
    "    \n",
    "    \n",
    "    def load_model(self, overwrite_epsilon=-1):\n",
    "        #set an value for epsilon to overwrite it\n",
    "        if overwrite_epsilon == -1:\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        else:\n",
    "            self.exploration_rate = overwrite_epsilon\n",
    "            \n",
    "        name = input(\"model file (in ./models):\")\n",
    "        self.model = keras.models.load_model(f\"./models/{name}.h5\")\n",
    "        self.target_model = self.model\n",
    "        pkl_name = input(\"replay memory file (enter x if none): \")\n",
    "        \n",
    "        if pkl_name == \"x\":\n",
    "            pass\n",
    "        else:\n",
    "            pass#self.memory = pickle.load(open(f\"./models/{pkl_name}.pkl\", \"rb\"))\n",
    "        print(f\"LOADED ./models/{name}.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb583f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
