{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aef52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "import lz4.frame as lz4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the agent\n",
    "class Agent: \n",
    "    def __init__(self, state_size, action_size,\n",
    "                 name=\"DeepQNetwork\",\n",
    "                 #anatomy of the hidden layers, must add the object from tf.keras.layers directly\n",
    "                 anatomy=[layers.Dense(24), layers.Dense(24)],\n",
    "                 #developer option, \n",
    "                 compile_model=False,\n",
    "                 lr=0.001,\n",
    "                 loss_function=keras.losses.MeanSquaredError(),\n",
    "                 optimizer=None, #defaults to adam. Is set below\n",
    "                 #debug options\n",
    "                 model_summary=True,\n",
    "                 model_verbose=0,\n",
    "                 #discount factor 0 <= gamma <= 1\n",
    "                 gamma=0.95,\n",
    "                 #exploration parameters\n",
    "                 linear_decrease=False,\n",
    "                 epsilon=1,\n",
    "                 epsilon_min=0.0001,\n",
    "                 epsilon_decay=0.995,\n",
    "                 #replay options\n",
    "                 batch_size=32,\n",
    "                 max_memory_size=1e6,\n",
    "                 compress_memory=False, #if enabled, 10% performance penalty for larger replay buffer EXP\n",
    "                 state_dtype=np.uint8,\n",
    "                 dtype_info_helper=np.iinfo,\n",
    "                 disable_double=False, #if true, acts like a single dqn\n",
    "                ):\n",
    "        \n",
    "        #model parameters\n",
    "        self.model_anatomy = anatomy \n",
    "        self.loss_function = loss_function\n",
    "        self.default_name = name\n",
    "        self.disable_double = disable_double\n",
    "        self.compile_model = compile_model\n",
    "        \n",
    "        #tracking vars\n",
    "        self.gradient_updates = 0\n",
    "        self.greedy_actions = 0\n",
    "        self.exploration_actions = 0\n",
    "        self.target_updates = 0\n",
    "        \n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.max_memory_size = max_memory_size\n",
    "        \n",
    "        #set default optimizer to Adam, if not provided\n",
    "        if optimizer != None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = Adam(learning_rate=lr, clipnorm=1.0)\n",
    "            print(f\"Optimizer Adam: {lr=}\")\n",
    "            \n",
    "        #initiate memory\n",
    "        self.compress_memory = compress_memory\n",
    "        self.state_dtype = state_dtype\n",
    "        self.dtype_info = dtype_info_helper(state_dtype)\n",
    "        self.action_hist = []\n",
    "        self.state_hist = []\n",
    "        self.state_next_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.done_hist = []\n",
    "        \n",
    "        #hyperparameters\n",
    "        self.learning_rate = lr\n",
    "        self.gamma = gamma\n",
    "        self.linear_decrease = linear_decrease\n",
    "        self.exploration_rate = epsilon\n",
    "        self.exploration_min = epsilon_min\n",
    "        self.exploration_decay = epsilon_decay\n",
    "        self.sample_batch_size = 32\n",
    "        \n",
    "        #debug options\n",
    "        self.model_summary = model_summary\n",
    "        self.model_verbose = model_verbose\n",
    "        \n",
    "        #create model\n",
    "        self.model = self._build_model(name=\"live_model\")\n",
    "        if not disable_double:\n",
    "            self.target_model = self._build_model(name=\"target_model\")\n",
    "        else:\n",
    "            self.target_model = self.model\n",
    "            \n",
    "        print(self.__str__())\n",
    "            \n",
    "    def __str__(self):\n",
    "        #useful for debugging\n",
    "        memory_ratio = (len(self.done_hist) / self.max_memory_size)*100 \n",
    "        ret_string = f\"\"\"\n",
    "                    NAME: {self.default_name}\n",
    "                    INPUT SHAPE: {self.state_size}, OUTPUT SHAPE: {self.action_size}\n",
    "                    OPTIMIZER: {type(self.optimizer)}\n",
    "                    LOSS FUNCTION: {type(self.loss_function)}\n",
    "                    LEARNING RATE: {self.learning_rate}\n",
    "                    TARGET_NETWORK: enabled-> {not self.disable_double}\n",
    "                                    target_updates->{self.target_updates}\n",
    "                    MEMORY: {len(self.done_hist)}/{self.max_memory_size:0.0f} Â¦ {memory_ratio:0.2f}%\n",
    "                    EPSILON: {self.exploration_rate:0.6f}\n",
    "                             min-> {self.exploration_min}\n",
    "                    ACTIONS TAKEN:  greedy-> {self.greedy_actions}\n",
    "                                    exploration-> {self.exploration_actions}\n",
    "                    REPLAY: batch_size-> {self.sample_batch_size}\n",
    "                            gamma-> {self.gamma}\n",
    "                            gradient_updates-> {self.gradient_updates}\n",
    "                    \"\"\"\n",
    "        return ret_string\n",
    "    \n",
    "    def _build_model(self, name=None):\n",
    "        #input layer\n",
    "        inputs = layers.Input(shape=self.state_size)\n",
    "        \n",
    "        #input layer\n",
    "        x = self.model_anatomy[0](inputs)\n",
    "        #create hidden layers\n",
    "        for layer in self.model_anatomy[1:]:\n",
    "            x = layer(x)\n",
    "        #output layer\n",
    "        outputs = layers.Dense(self.action_size, activation=\"linear\")(x)\n",
    "        \n",
    "        #create model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        \n",
    "        if self.compile_model:\n",
    "            model.compile(optimizer=self.optimizer,\n",
    "                         loss=self.loss_function)\n",
    "        #model summary if enabled\n",
    "        if self.model_summary and self.compile_model: model.summary()\n",
    "        return model\n",
    "    \n",
    "    def update_memory(self, state, reward, action, state_next, done):\n",
    "        #Format of step: [state(t), reward(t+1), action(t), state(t+1), done?]\n",
    "        \n",
    "        self.reward_hist.append(reward)\n",
    "        self.action_hist.append(action)\n",
    "        self.done_hist.append(done)\n",
    "        \n",
    "        state = self.replay_compress(state) if self.compress_memory else state\n",
    "        state_next = self.replay_compress(state_next) if self.compress_memory else state_next\n",
    "        \n",
    "        self.state_hist.append(state)\n",
    "        self.state_next_hist.append(state_next)\n",
    "        \n",
    "        if len(self.state_hist) > self.max_memory_size:\n",
    "            del self.state_hist[:1]\n",
    "            del self.reward_hist[:1]\n",
    "            del self.action_hist[:1]\n",
    "            del self.state_next_hist[:1]\n",
    "            del self.done_hist[:1]\n",
    "            \n",
    "    def pick_action(self, state):\n",
    "        if np.random.rand(1)[0] < self.exploration_rate:\n",
    "            #return random move \n",
    "            self.exploration_actions += 1\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            q_values = self.predict(tf.convert_to_tensor(state))[0]\n",
    "            #return action with the highest expected reward\n",
    "            return_val = np.array(tf.argmax(q_values))\n",
    "            self.greedy_actions += 1\n",
    "            return return_val\n",
    "\n",
    "    def predict(self, state, main=True):\n",
    "        #print(state.shape)\n",
    "        if main:\n",
    "            return self.model.predict(state, verbose=self.model_verbose)\n",
    "        else:\n",
    "            return self.target_model.predict(state, verbose=self.model_verbose)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_updates += 1\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    #compress states into a bytes\n",
    "    def replay_compress(self, replay_obj):\n",
    "        #replay_obj *= self.dtype_info.max #scale [0-1] to [0-max_dtype]\n",
    "        replay_obj = replay_obj.astype(dtype=self.state_dtype)\n",
    "        flattened = replay_obj.flatten(\"F\")\n",
    "        obj_bytes = replay_obj.tobytes()\n",
    "        return lz4f.compress(obj_bytes)\n",
    "    \n",
    "    #helper function for sampling the minibatch from the replay memory\n",
    "    def replay_unpack(self, replay_obj):\n",
    "        bytes_decompressed = lz4f.decompress(replay_obj)\n",
    "        arr = np.frombuffer(bytes_decompressed, dtype=self.state_dtype)\n",
    "        reshaped = np.reshape(arr, self.state_size)\n",
    "        #reshaped_adj = reshaped / self.dtype_info.max\n",
    "        return reshaped\n",
    "    \n",
    "    def replay(self, debug=False):\n",
    "        if len(self.state_hist) < self.sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        self.gradient_updates += 1\n",
    "        #replay\n",
    "        #samples random experiences from memory\n",
    "        indices = np.random.choice(range(len(self.done_hist)), size=self.sample_batch_size)\n",
    "        \n",
    "        if debug: \n",
    "            print(\"++++++++++++\")\n",
    "            #print(f\"{indices=}\")\n",
    "        \n",
    "        if self.compress_memory:\n",
    "            state_batch = np.array([\n",
    "                self.replay_unpack(self.state_hist[i]) for i in indices])\n",
    "            next_state_batch = np.array([\n",
    "                self.replay_unpack(self.state_next_hist[i]) for i in indices])\n",
    "        else:\n",
    "            state_batch = np.array([\n",
    "                self.state_hist[i] for i in indices])\n",
    "            next_state_batch = np.array([\n",
    "                self.state_next_hist[i] for i in indices])\n",
    "        \n",
    "        action_batch = [self.action_hist[i] for i in indices]\n",
    "        reward_batch = [self.reward_hist[i] for i in indices]\n",
    "        done_batch = tf.convert_to_tensor(\n",
    "            [float(self.done_hist[i]) for i in indices]\n",
    "        )\n",
    "\n",
    "        #get future rewards from target model\n",
    "        future_rewards = self.target_model.predict(next_state_batch, verbose=self.model_verbose)\n",
    "        \n",
    "        #update q_values for every action in state\n",
    "        updated_q_values = reward_batch + (self.gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1))\n",
    "        #set reward to -1 if done\n",
    "        updated_q_values = updated_q_values * (1 - done_batch) - done_batch\n",
    "        \n",
    "        masks = tf.one_hot(action_batch, self.action_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = self.model(state_batch)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        if debug:\n",
    "            for i in range(2):\n",
    "                print(action_batch[i])\n",
    "                print(current_rewards[i])\n",
    "                print(tf.reduce_max(future_rewards, axis=1)[i])\n",
    "                print(updated_q_values[i])\n",
    "                print(\"-----\")\n",
    "            print(f\"{loss=}\")\n",
    " \n",
    "\n",
    "        #update exploration rate\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            if not self.linear_decrease:\n",
    "                self.exploration_rate *= self.exploration_decay\n",
    "            else:\n",
    "                self.exploration_rate -= self.exploration_decay\n",
    "    \n",
    "    def build_name(self, name=None):\n",
    "        if name != None:\n",
    "            name = name + \"-\"\n",
    "        else:\n",
    "            name = \"\"\n",
    "        now = datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        return f\"{self.default_name}-{name}{now}\"\n",
    "        \n",
    "    def save_model(self, name=\"\", save_memory=False):\n",
    "        self.model.save(f\"./models/{self.build_name(name=name)}.h5\")\n",
    "        if save_memory:\n",
    "            pass\n",
    "            #pickle.dump(self.memory, open(f\"./models/{self.build_name(name=name)}.pkl\", \"wb\"))\n",
    "        print(f\"MODEL SAVED AS ./models/{self.build_name(name=name)}.h5\")\n",
    "    \n",
    "    \n",
    "    def load_model(self, overwrite_epsilon=-1):\n",
    "        #set an value for epsilon to overwrite it\n",
    "        if overwrite_epsilon == -1:\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        else:\n",
    "            self.exploration_rate = overwrite_epsilon\n",
    "            \n",
    "        name = input(\"model file (in ./models):\")\n",
    "        self.model = keras.models.load_model(f\"./models/{name}.h5\")\n",
    "        self.target_model = self.model\n",
    "        pkl_name = input(\"replay memory file (enter x if none): \")\n",
    "        \n",
    "        if pkl_name == \"x\":\n",
    "            pass\n",
    "        else:\n",
    "            pass#self.memory = pickle.load(open(f\"./models/{pkl_name}.pkl\", \"rb\"))\n",
    "        print(f\"LOADED ./models/{name}.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb583f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741281d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad5417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
