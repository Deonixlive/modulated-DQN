{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aef52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "import lz4.frame as lz4f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cant do nested imports, copy here \n",
    "def normc_initializer(std: float = 1.0):\n",
    "    def _initializer(shape, dtype=None):\n",
    "        out = np.random.randn(*shape).astype(\n",
    "            dtype.name if hasattr(dtype, \"name\") else dtype or np.float32\n",
    "        )\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "class DuelingModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, num_outputs):\n",
    "        \n",
    "        super().__init__(self)\n",
    "\n",
    "#         print(obs_space)\n",
    "        self.inputs = tf.keras.layers.Input(shape=input_shape,\n",
    "                                           name=\"input_layer\")\n",
    "        layer1 = tf.keras.layers.Conv2D(32, 8, strides=4,\n",
    "                                       name=\"layer1\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(self.inputs)\n",
    "        layer2 = tf.keras.layers.Conv2D(64, 4, strides=2,\n",
    "                                       name=\"layer2\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(layer1)\n",
    "        layer3 = tf.keras.layers.Conv2D(64, 3, strides=1,\n",
    "                                       name=\"layer3\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(layer2)\n",
    "        flatten = tf.keras.layers.Flatten()(layer3)\n",
    "        #split stream\n",
    "        value_stream = dense = tf.keras.layers.Dense(512,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name=\"value_stream\",\n",
    "                                        kernel_initializer=normc_initializer(1.0)\n",
    "                                        )(flatten)\n",
    "        \n",
    "        advantage_stream = tf.keras.layers.Dense(512,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name=\"advantage_stream\",\n",
    "                                        kernel_initializer=normc_initializer(1.0)\n",
    "                                        )(flatten)\n",
    "        value_out = dense = tf.keras.layers.Dense(1,\n",
    "                                        activation=None,\n",
    "                                        name=\"value_out\",\n",
    "                                        kernel_initializer=normc_initializer(1.0)\n",
    "                                        )(flatten)\n",
    "        advantage_out = dense = tf.keras.layers.Dense(num_outputs,\n",
    "                                        activation=None,\n",
    "                                        name=\"advantage_out\",\n",
    "                                        kernel_initializer=normc_initializer(1.0)\n",
    "                                        )(flatten)\n",
    "        #Version with advantage mean, to solve unidentifiabilty\n",
    "        self.q_out = (value_out + (advantage_out - tf.math.reduce_mean(advantage_out, axis=1, keepdims=True)))\n",
    "\n",
    "        #doesnt output the actual q-values. this is handled in the q-head\n",
    "        self.base_model = tf.keras.Model(inputs=self.inputs, outputs=self.q_out)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.base_model(inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class priorisedMemory:  # stored as ( s, a, r, s') in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.8\n",
    "    beta = 0.3\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "# Source: https://github.com/rlcode/per/blob/master/SumTree.py\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the agent\n",
    "class Agent: \n",
    "    def __init__(self, state_size, action_size,\n",
    "                 name=\"modulatedDeepQNetwork\",\n",
    "                 #anatomy of the hidden layers, must add the object from tf.keras.layers directly\n",
    "                 anatomy=[layers.Dense(24), layers.Dense(24)], #DEPRECATED WITH DUELING SETUP\n",
    "                 #developer option, \n",
    "                 compile_model=False,\n",
    "                 lr=0.001,\n",
    "                 loss_function=keras.losses.MeanSquaredError(),\n",
    "                 optimizer=None, #defaults to adam. Is set below\n",
    "                 #debug options\n",
    "                 model_summary=True,\n",
    "                 model_verbose=0,\n",
    "                 #discount factor 0 <= gamma <= 1\n",
    "                 gamma=0.95,\n",
    "                 #exploration parameters\n",
    "                 linear_decrease=False,\n",
    "                 epsilon=1,\n",
    "                 epsilon_min=0.0001,\n",
    "                 epsilon_decay=0.995,\n",
    "                 #replay options\n",
    "                 batch_size=32,\n",
    "                 max_memory_size=int(1e6),\n",
    "                 compress_memory=False, #if enabled, 5% performance penalty for larger replay buffer EXP\n",
    "                 state_dtype=np.uint8,\n",
    "                 dtype_info_helper=np.iinfo,\n",
    "                 disable_double=False, #if true, acts like a single dqn\n",
    "                ):\n",
    "        \n",
    "        #model parameters\n",
    "        self.model_anatomy = anatomy \n",
    "        self.loss_function = loss_function\n",
    "        self.default_name = name\n",
    "        self.disable_double = disable_double\n",
    "        self.compile_model = compile_model\n",
    "        \n",
    "        #tracking vars\n",
    "        self.gradient_updates = 0\n",
    "        self.greedy_actions = 0\n",
    "        self.exploration_actions = 0\n",
    "        self.target_updates = 0\n",
    "        \n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.max_memory_size = max_memory_size\n",
    "        \n",
    "        #set default optimizer to Adam, if not provided\n",
    "        if optimizer != None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = Adam(learning_rate=lr, clipnorm=1.0)\n",
    "            print(f\"Optimizer Adam: {lr=}\")\n",
    "            \n",
    "        #initiate memory\n",
    "        self.compress_memory = compress_memory\n",
    "        self.state_dtype = state_dtype\n",
    "        self.dtype_info = dtype_info_helper(state_dtype)\n",
    "        self.memory_buffer = priorisedMemory(self.max_memory_size)\n",
    "        \n",
    "        #hyperparameters\n",
    "        self.learning_rate = lr\n",
    "        self.gamma = gamma\n",
    "        self.linear_decrease = linear_decrease\n",
    "        self.exploration_rate = epsilon\n",
    "        self.exploration_min = epsilon_min\n",
    "        self.exploration_decay = epsilon_decay\n",
    "        self.sample_batch_size = 32\n",
    "        \n",
    "        #debug options\n",
    "        self.model_summary = model_summary\n",
    "        self.model_verbose = model_verbose\n",
    "        \n",
    "        #create model\n",
    "        self.model = DuelingModel(self.state_size, self.action_size)\n",
    "        if not disable_double:\n",
    "            self.target_model = DuelingModel(self.state_size, self.action_size)\n",
    "        else:\n",
    "            self.target_model = self.model\n",
    "            \n",
    "        print(self.__str__())\n",
    "            \n",
    "    def __str__(self):\n",
    "        #useful for debugging\n",
    "        ret_string = f\"\"\"\n",
    "                    NAME: {self.default_name}\n",
    "                    INPUT SHAPE: {self.state_size}, OUTPUT SHAPE: {self.action_size}\n",
    "                    OPTIMIZER: {type(self.optimizer)}\n",
    "                    LOSS FUNCTION: {type(self.loss_function)}\n",
    "                    LEARNING RATE: {self.learning_rate}\n",
    "                    TARGET_NETWORK: enabled-> {not self.disable_double}\n",
    "                                    target_updates->{self.target_updates}\n",
    "                    EPSILON: {self.exploration_rate:0.6f}\n",
    "                             min-> {self.exploration_min}\n",
    "                    ACTIONS TAKEN:  greedy-> {self.greedy_actions}\n",
    "                                    exploration-> {self.exploration_actions}\n",
    "                    REPLAY: batch_size-> {self.sample_batch_size}\n",
    "                            gamma-> {self.gamma}\n",
    "                            gradient_updates-> {self.gradient_updates}\n",
    "                    \"\"\"\n",
    "        return ret_string\n",
    "    \n",
    "    #NOT USED WITH DUEALING SETUP\n",
    "    def _build_model(self, name=None):\n",
    "        #input layer\n",
    "        inputs = layers.Input(shape=self.state_size)\n",
    "        \n",
    "        #input layer\n",
    "        x = self.model_anatomy[0](inputs)\n",
    "        #create hidden layers\n",
    "        for layer in self.model_anatomy[1:]:\n",
    "            x = layer(x)\n",
    "        #output layer\n",
    "        outputs = layers.Dense(self.action_size, activation=\"linear\")(x)\n",
    "        \n",
    "        #create model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        \n",
    "        if self.compile_model:\n",
    "            model.compile(optimizer=self.optimizer,\n",
    "                         loss=self.loss_function)\n",
    "        #model summary if enabled\n",
    "        if self.model_summary and self.compile_model: model.summary()\n",
    "        return model\n",
    "    \n",
    "    def update_memory(self, state, reward, action, state_next, done):\n",
    "        #Format of step: [state(t), reward(t+1), action(t), state(t+1), done?]\n",
    "        action_choice = np.argmax(self.predict(tf.expand_dims(state, 0))[0])\n",
    "        \n",
    "        td = (reward + self.gamma*self.predict(tf.expand_dims(state_next, 0)\n",
    "                                              )[0][action_choice] \n",
    "                - self.predict(tf.expand_dims(state, 0))[0][action])\n",
    "        \n",
    "        state = self.replay_compress(state) if self.compress_memory else state\n",
    "        state_next = self.replay_compress(state_next) if self.compress_memory else state_next\n",
    "        \n",
    "        self.memory_buffer.add(error=td, sample=(state, reward, action, state_next, done))\n",
    "        \n",
    "    def pick_action(self, state):\n",
    "        if np.random.rand(1)[0] < self.exploration_rate:\n",
    "            #return random move \n",
    "            self.exploration_actions += 1\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            q_values = self.predict(tf.convert_to_tensor(state))[0]\n",
    "            #return action with the highest expected reward\n",
    "            return_val = np.array(tf.argmax(q_values))\n",
    "            self.greedy_actions += 1\n",
    "            return return_val\n",
    "\n",
    "    def predict(self, state, main=True):\n",
    "        #print(state.shape)\n",
    "        if main:\n",
    "            return self.model.predict(state, verbose=self.model_verbose)\n",
    "        else:\n",
    "            return self.target_model.predict(state, verbose=self.model_verbose)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_updates += 1\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    #compress states into a bytes\n",
    "    def replay_compress(self, replay_obj):\n",
    "        #replay_obj *= self.dtype_info.max #scale [0-1] to [0-max_dtype]\n",
    "        replay_obj = replay_obj.astype(dtype=self.state_dtype)\n",
    "        flattened = replay_obj.flatten(\"F\")\n",
    "        obj_bytes = replay_obj.tobytes()\n",
    "        return lz4f.compress(obj_bytes)\n",
    "    \n",
    "    #helper function for sampling the minibatch from the replay memory\n",
    "    def replay_unpack(self, replay_obj):\n",
    "        bytes_decompressed = lz4f.decompress(replay_obj)\n",
    "        arr = np.frombuffer(bytes_decompressed, dtype=self.state_dtype)\n",
    "        reshaped = np.reshape(arr, self.state_size)\n",
    "        #reshaped_adj = reshaped / self.dtype_info.max\n",
    "        return reshaped\n",
    "    \n",
    "    def replay(self, debug=False):\n",
    "        if self.memory_buffer.tree.n_entries < self.sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        batch, indices, is_weight = self.memory_buffer.sample(self.sample_batch_size)\n",
    "        \n",
    "        self.gradient_updates += 1\n",
    "        #replay\n",
    "        \n",
    "        state_raw_batch = [x[0] for x in batch]\n",
    "        reward_batch = [x[1] for x in batch]\n",
    "        action_batch = [x[2] for x in batch]\n",
    "        state_next_raw_batch = [x[3] for x in batch]\n",
    "        done_batch = [x[4] for x in batch]\n",
    "        \n",
    "\n",
    "        if debug: \n",
    "            print(\"++++++++++++\")\n",
    "            #print(f\"{indices=}\")\n",
    "        \n",
    "        if self.compress_memory:\n",
    "            state_batch = np.array([\n",
    "                self.replay_unpack(state_raw_batch[i]) for i in range(self.sample_batch_size)])\n",
    "            next_state_batch = np.array([\n",
    "                self.replay_unpack(state_next_raw_batch[i]) for i in range(self.sample_batch_size)])\n",
    "        else:\n",
    "            state_batch = np.array([\n",
    "                self.state_raw_batch[i] for i in range(self.sample_batch_size)])\n",
    "            next_state_batch = np.array([\n",
    "                self.state_next_raw_batch[i] for i in range(self.sample_batch_size)])\n",
    "\n",
    "        done_batch = tf.convert_to_tensor(\n",
    "            [float(done_batch[i]) for i in range(self.sample_batch_size)]\n",
    "        )\n",
    "\n",
    "        #get future rewards from target model\n",
    "        future_rewards = self.target_model.predict(next_state_batch, verbose=self.model_verbose)\n",
    "        \n",
    "        #update q_values for every action in state\n",
    "        target_vals = reward_batch + (self.gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1))\n",
    "        #set reward to -1 if done\n",
    "        target_vals = target_vals * (1 - done_batch) - done_batch\n",
    "        \n",
    "        \n",
    "        masks = tf.one_hot(action_batch, self.action_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = self.model(state_batch)\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = self.loss_function(target_vals, q_action)\n",
    "\n",
    "        # Optimization\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        if debug:\n",
    "            for i in range(2):\n",
    "                print(action_batch[i])\n",
    "                print(current_rewards[i])\n",
    "                print(tf.reduce_max(future_rewards, axis=1)[i])\n",
    "                print(updated_q_values[i])\n",
    "                print(\"-----\")\n",
    "            print(f\"{loss=}\")\n",
    " \n",
    "\n",
    "        #update exploration rate\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            if not self.linear_decrease:\n",
    "                self.exploration_rate *= self.exploration_decay\n",
    "            else:\n",
    "                self.exploration_rate -= self.exploration_decay\n",
    "    \n",
    "    def build_name(self, name=None):\n",
    "        if name != None:\n",
    "            name = name + \"-\"\n",
    "        else:\n",
    "            name = \"\"\n",
    "        now = datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        return f\"{self.default_name}-{name}{now}\"\n",
    "        \n",
    "    def save_model(self, name=\"\", save_memory=False):\n",
    "        self.model.save(f\"./models/{self.build_name(name=name)}.h5\")\n",
    "        if save_memory:\n",
    "            pass\n",
    "            #pickle.dump(self.memory, open(f\"./models/{self.build_name(name=name)}.pkl\", \"wb\"))\n",
    "        print(f\"MODEL SAVED AS ./models/{self.build_name(name=name)}.h5\")\n",
    "    \n",
    "    \n",
    "    def load_model(self, overwrite_epsilon=-1):\n",
    "        #set an value for epsilon to overwrite it\n",
    "        if overwrite_epsilon == -1:\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        else:\n",
    "            self.exploration_rate = overwrite_epsilon\n",
    "            \n",
    "        name = input(\"model file (in ./models):\")\n",
    "        self.model = keras.models.load_model(f\"./models/{name}.h5\")\n",
    "        self.target_model = self.model\n",
    "        pkl_name = input(\"replay memory file (enter x if none): \")\n",
    "        \n",
    "        if pkl_name == \"x\":\n",
    "            pass\n",
    "        else:\n",
    "            pass#self.memory = pickle.load(open(f\"./models/{pkl_name}.pkl\", \"rb\"))\n",
    "        print(f\"LOADED ./models/{name}.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb583f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741281d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad5417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
