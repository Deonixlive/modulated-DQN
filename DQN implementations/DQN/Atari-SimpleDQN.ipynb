{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from DQN.SimpleDQN import Agent\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import losses\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5472f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#agent and environment optimized for Atari games\n",
    "#https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "class AtariGame:\n",
    "    def __init__(self, env_name, render_mode=None, name=None):\n",
    "        self.name = name\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes          = 10000\n",
    "        self.env_name          = env_name\n",
    "        self.env               = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n",
    "        self.frame_skip        = 4 #skip every 4th frame\n",
    "        #84x84 greyscale\n",
    "        self.reduzed_size      = (84, 84, self.frame_skip) #, greyscale(1), 84x84, 4 Pictures, \n",
    "        self.state_size        = self.reduzed_size\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        self.termination_index = 10000\n",
    "        self.agent             = Agent(self.state_size, self.action_size, \n",
    "                                       #Parameters taken from Deepmind Breakout AI\n",
    "                                       #input model layers as keras.layers objects\n",
    "                                       anatomy=[layers.Conv2D(16,8,strides=4,activation=activations.relu),\n",
    "                                                layers.Conv2D(32,4,strides=2,activation=activations.relu),\n",
    "                                                layers.Conv2D(64,3,strides=1,activation=activations.relu),\n",
    "                                                layers.Flatten(),\n",
    "                                                layers.Dense(512, activation=activations.relu)],\n",
    "                                       name=f\"{self.env_name}-DQN\",\n",
    "                                       linear_decrease=True,\n",
    "                                       #epsilon=0.1, #DEBUG\n",
    "                                       epsilon_decay=0.9/31000,\n",
    "                                       epsilon_min=0.1,\n",
    "                                       model_verbose=0,\n",
    "                                       lr = 0.00025,\n",
    "                                       gamma= 0.99,\n",
    "                                       loss = losses.Huber(),\n",
    "                                       max_memory_size=1.9e5\n",
    "                                       )\n",
    "        print(self.state_size, self.action_size)\n",
    "        self.history           = []\n",
    "        self.env.unwrapped.get_action_meanings()\n",
    "    def reduce(self, state):\n",
    "        res = cv2.resize(state, dsize=(84,110))\n",
    "        #print(res.shape)\n",
    "        margin = 110 - 84\n",
    "        reduced = res[13:110-13]\n",
    "        return np.reshape(reduced, (84, 84))\n",
    "        \n",
    "    def run(self, load_model = False, skip_training=False, overwrite_epsilon=-1, save=True, logs=False):\n",
    "        #LOGS ARE ALWAYS ENABLED\n",
    "        logging.basicConfig(filename=f\"models/{self.env_name}-DQN.log\",\n",
    "                level=logging.INFO,\n",
    "                format='%(levelname)s: %(asctime)s %(message)s',\n",
    "                datefmt='%d/%m/%Y %I:%M:%S')\n",
    "        \n",
    "        self.env._max_episode_steps = 20000\n",
    "        \n",
    "        #self.env.render()\n",
    "        \n",
    "        #if true, try to load existing model\n",
    "        if load_model:\n",
    "            self.agent.load_model(overwrite_epsilon=overwrite_epsilon)\n",
    "        try:\n",
    "            training_batches = 0\n",
    "            for index_episode in range(self.episodes):\n",
    "                state = self.env.reset()\n",
    "                state = self.reduce(state)\n",
    "                done = False\n",
    "                index = 1\n",
    "                state_arr = []\n",
    "                next_state_arr = []\n",
    "                score = 0\n",
    "                \n",
    "                #logs performance\n",
    "                q = 0\n",
    "                q_n = 0\n",
    "                #Real time Array with the last frames, used to generate an action\n",
    "                state_action_arr = np.array([np.zeros([84,84]) for i in range(self.frame_skip)]) \n",
    "                \n",
    "                while not done:\n",
    "                    #print(\"CHOOSE ACTION\")\n",
    "                    #logging.info(str(state.shape))\n",
    "                    #update state array queue\n",
    "                    state_action_arr = np.roll(state_action_arr, shift=1)\n",
    "                    state_action_arr[0] = state\n",
    "                    \n",
    "                    #first frames actions are random\n",
    "    \n",
    "                    if index > self.frame_skip:\n",
    "                        #print(index)\n",
    "                        #print(state_action_arr)\n",
    "                        action = self.agent.pick_action(np.reshape(state_action_arr, (1, *self.reduzed_size)))\n",
    "                    else:\n",
    "                        #print(\"INIT\")\n",
    "                        action = np.random.choice(self.agent.action_size)\n",
    "                    #print(action)\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    score += reward\n",
    "                    #print(\"action confirmed\")\n",
    "                    #next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    next_state = self.reduce(next_state)\n",
    "                    \n",
    "                    #print(state.shape)\n",
    "                    next_state_arr.append(next_state)\n",
    "                    #print(\"FRAME\")\n",
    "                    \n",
    "                    if (index % self.frame_skip == 0) and not skip_training:\n",
    "                        #skip training on the first frameskip\n",
    "                        if index > self.frame_skip:\n",
    "                            #print(\"TRAIN\")\n",
    "                            #add q_value\n",
    "                            #print(state_arr.shape)\n",
    "                            q += np.amax(self.agent.predict(state_arr))\n",
    "                            q_n += 1\n",
    "                            self.agent.update_memory((state_arr, action, reward, \n",
    "                                                      np.reshape(np.array(next_state_arr),\n",
    "                                                                 (1, *self.reduzed_size)),\n",
    "                                                                done))\n",
    "                            #replay \"training\"\n",
    "                            training_batches += 1\n",
    "                            self.agent.replay()\n",
    "                        #print(np.array(next_state_arr).shape)\n",
    "                        \n",
    "                        \n",
    "                        state_arr = np.reshape(np.array(next_state_arr), (1, *self.reduzed_size))\n",
    "                        next_state_arr = []\n",
    "                        \n",
    "                    state = next_state\n",
    "                    \n",
    "                    #print(index, done, action)\n",
    "                    if index > self.termination_index:\n",
    "                        done = True\n",
    "                    index += 1\n",
    "                    \n",
    "                print(f\"Episode: {index_episode:-10}\")\n",
    "                print(f\"Score: {score:-12}\")\n",
    "                print(f\"Epsilon: {self.agent.exploration_rate}\")\n",
    "                print(\"\".join([\"_\" for i in range(10)]))\n",
    "                \n",
    "                if logs:\n",
    "                    logging.info(f\"EPISODE: {index_episode}\")\n",
    "                    logging.info(f\"FRAMES TRAINED: {training_batches * 32}\")\n",
    "                    logging.info(f\"AVG Q VALUE: {(q / q_n):.5f}\")\n",
    "                    logging.info(f\"SCORE: {score}\")\n",
    "                    logging.info(f\"DURATION (STEPS): {index}\")\n",
    "                    logging.info(f\"EPSILON: {self.agent.exploration_rate:.5f}\")\n",
    "                    logging.info(f\"MEMORY SIZE: {len(self.agent.memory)}\")\n",
    "                    logging.info(\"\".join([\"-\" for i in range(12)]))\n",
    "\n",
    "                self.history.append(index)\n",
    "                \n",
    "                #save every 0.5k episodes\n",
    "                if index_episode % 500 == 0 and save and index_episode != 0:\n",
    "                    self.agent.save_model(name=\"EP\"+str(index_episode))\n",
    "        except:\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            logging.exception(\"An error has occured\")\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "        finally:\n",
    "            #save model upon interrupting\n",
    "            if logs:\n",
    "                logging.info(f\"TRAINING FINISHED AFTER {training_batches} BATCHES\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "                if save:\n",
    "                    model_name = self.agent.build_name(name=\"FINISHED\")\n",
    "                    logging.info(f\"MODEL NAME: {model_name}\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            #print(training_batches)\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"FINISHED\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #render modes: \"human\", None\n",
    "    atari = AtariGame(\"ALE/Breakout-v5\", render_mode=None)\n",
    "    atari.run(load_model=False, skip_training=False, overwrite_epsilon=-1, save=True, logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffceac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
