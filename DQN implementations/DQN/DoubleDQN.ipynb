{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79aef52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#for AMD gpus\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "235f5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the agent\n",
    "class Agent: \n",
    "    def __init__(self, state_size, action_size,\n",
    "                 name=\"DeepQNetwork\",\n",
    "                 #anatomy of the hidden layers, must add the object from tf.keras.layers directly\n",
    "                 anatomy=[layers.Dense(24), layers.Dense(24)],\n",
    "                 #developer option, \n",
    "                 compile_model=True,\n",
    "                 lr=0.001,\n",
    "                 loss_function=keras.losses.MeanSquaredError(),\n",
    "                 optimizer=None, #defaults to adam. Is set below\n",
    "                 #debug options\n",
    "                 model_summary=True,\n",
    "                 model_verbose=0,\n",
    "                 #discount factor 0 <= gamma <= 1\n",
    "                 gamma=0.95,\n",
    "                 #exploration parameters\n",
    "                 linear_decrease=False,\n",
    "                 epsilon=1,\n",
    "                 epsilon_min=0.0001,\n",
    "                 epsilon_decay=0.995,\n",
    "                 #replay options\n",
    "                 batch_size=32,\n",
    "                 max_memory_size=1e6,\n",
    "                 disable_double=False, #if true, acts like a single dqn\n",
    "                ):\n",
    "        \n",
    "        #model parameters\n",
    "        self.model_anatomy = anatomy \n",
    "        self.default_name = name\n",
    "        self.disable_double = disable_double\n",
    "        #tracking vars\n",
    "        self.gradient_updates = 0\n",
    "        self.greedy_actions = 0\n",
    "        self.exploration_actions = 0\n",
    "        self.target_updates = 0\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.max_memory_size = max_memory_size\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        #set default optimizer to Adam, if not provided\n",
    "        if optimizer != None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = Adam(learning_rate=lr, clipnorm=1.0)\n",
    "            print(f\"Optimizer Adam: {lr=}\")\n",
    "            \n",
    "        #initiate memory\n",
    "        self.action_hist = []\n",
    "        self.state_hist = []\n",
    "        self.state_next_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.done_hist = []\n",
    "        \n",
    "        \n",
    "        #hyperparameters\n",
    "        self.learning_rate = lr\n",
    "        self.gamma = gamma\n",
    "        self.linear_decrease = linear_decrease\n",
    "        self.exploration_rate = epsilon\n",
    "        self.exploration_min = epsilon_min\n",
    "        self.exploration_decay = epsilon_decay\n",
    "        self.sample_batch_size = 32\n",
    "        \n",
    "        #debug options\n",
    "        self.model_summary = model_summary\n",
    "        self.model_verbose = model_verbose\n",
    "        \n",
    "        #create model\n",
    "        self.model = self._build_model(name=\"live_model\")\n",
    "        if not disable_double:\n",
    "            self.target_model = self._build_model(name=\"target_model\")\n",
    "        else:\n",
    "            self.target_model = self.model\n",
    "            \n",
    "    def __str__(self):\n",
    "        #useful for debugging\n",
    "        memory_ratio = (len(self.done_hist) / self.max_memory_size)*100 \n",
    "        ret_string = f\"\"\"\n",
    "                    NAME: {self.default_name}\n",
    "                    INPUT SHAPE: {self.state_size}, OUTPUT SHAPE: {self.action_size}\n",
    "                    OPTIMIZER: {type(self.optimizer)}\n",
    "                    LOSS FUNCTION: {type(self.loss_function)}\n",
    "                    LEARNING RATE: {self.learning_rate}\n",
    "                    TARGET_NETWORK: enabled-> {not self.disable_double}\n",
    "                                    target_updates->{self.target_updates}\n",
    "                    MEMORY: {len(self.done_hist)}/{self.max_memory_size:0.0f} ¦ {memory_ratio:0.2f}%\n",
    "                    EPSILON: {self.exploration_rate:0.6f}\n",
    "                             min-> {self.exploration_min}\n",
    "                    ACTIONS TAKEN:  greedy-> {self.greedy_actions}\n",
    "                                    exploration-> {self.exploration_actions}\n",
    "                    REPLAY: batch_size-> {self.sample_batch_size}\n",
    "                            gamma-> {self.gamma}\n",
    "                            gradient_updates-> {self.gradient_updates}\n",
    "                    \"\"\"\n",
    "        return ret_string\n",
    "    \n",
    "    def _build_model(self, name=None):\n",
    "        #input layer\n",
    "        inputs = keras.Input(shape=self.state_size)\n",
    "        \n",
    "        #input layer\n",
    "        x = self.model_anatomy[0](inputs)\n",
    "        #create hidden layers\n",
    "        for layer in self.model_anatomy[1:]:\n",
    "            x = layer(x)\n",
    "        #output layer\n",
    "        outputs = layers.Dense(self.action_size, activation = \"linear\")(x)\n",
    "        \n",
    "        #create model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.compile(optimizer=self.optimizer,\n",
    "                     loss=self.loss_function)\n",
    "        #model summary if enabled\n",
    "        if self.model_summary: model.summary()\n",
    "        return model\n",
    "    \n",
    "    def update_memory(self, state, reward, action, state_next, done):\n",
    "        #Format of step: [state(t), reward(t+1), action(t), state(t+1), done?]\n",
    "        \n",
    "        self.state_hist.append(state)\n",
    "        self.reward_hist.append(reward)\n",
    "        self.action_hist.append(action)\n",
    "        self.state_next_hist.append(state_next)\n",
    "        self.done_hist.append(done)\n",
    "        \n",
    "        if len(self.state_hist) > self.max_memory_size:\n",
    "            del self.state_hist[:1]\n",
    "            del self.reward_hist[:1]\n",
    "            del self.action_hist[:1]\n",
    "            del self.state_next_hist[:1]\n",
    "            del self.done_hist[:1]\n",
    "            \n",
    "    def pick_action(self, state):\n",
    "        if np.random.rand(1)[0] < self.exploration_rate:\n",
    "            #return random move \n",
    "            self.exploration_actions += 1\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            q_values = self.predict(tf.convert_to_tensor(state))[0]\n",
    "            #return action with the highest expected reward\n",
    "            return_val = np.array(tf.argmax(q_values))\n",
    "            self.greedy_actions += 1\n",
    "            return return_val\n",
    "\n",
    "    def predict(self, state, main=True):\n",
    "        #print(state.shape)\n",
    "        if main:\n",
    "            return self.model.predict(state, verbose=self.model_verbose)\n",
    "        else:\n",
    "            return self.target_model.predict(state, verbose=self.model_verbose)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_updates += 1\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def replay(self, debug=False):\n",
    "        if len(self.state_hist) < self.sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        #replay\n",
    "        #samples random experiences from memory\n",
    "        indices = np.random.choice(range(len(self.done_hist)), size=self.sample_batch_size)\n",
    "        \n",
    "        if debug: \n",
    "            print(\"++++++++++++\")\n",
    "            #print(f\"{indices=}\")\n",
    "        \n",
    "        state_batch = np.array([self.state_hist[i] for i in indices])\n",
    "        next_state_batch = np.array([self.state_next_hist[i] for i in indices])\n",
    "        action_batch = [self.action_hist[i] for i in indices]\n",
    "        reward_batch = [self.reward_hist[i] for i in indices]\n",
    "        done_batch = tf.convert_to_tensor(\n",
    "            [float(self.done_hist[i]) for i in indices]\n",
    "        )\n",
    "\n",
    "        #get future rewards from target model\n",
    "        future_rewards = self.predict(tf.convert_to_tensor(next_state_batch), main=False)\n",
    "        \n",
    "        #update q_values for every action in state\n",
    "        updated_q_values = reward_batch + (self.gamma * tf.reduce_max(future_rewards, axis=1))\n",
    "        #set reward to -1 if done\n",
    "        updated_q_values = (updated_q_values * (1 - done_batch)) - done_batch\n",
    "         \n",
    "        #get current rewards\n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        current_rewards = self.predict(state_batch)\n",
    "        \n",
    "        #generate indices to replace\n",
    "        mask = [[i, action_batch[i]] for i in range(self.sample_batch_size)]\n",
    "        #update current rewards to \n",
    "        target_values = tf.tensor_scatter_nd_update(current_rewards, mask, updated_q_values)\n",
    "        \n",
    "        if debug:\n",
    "            print(action_batch[0])\n",
    "            print(current_rewards[0])\n",
    "            print(tf.reduce_max(future_rewards, axis=1)[0])\n",
    "            print(updated_q_values[0])\n",
    "            print(target_values[0])\n",
    "            print(\"++++\")\n",
    "            print(action_batch[1])\n",
    "            print(current_rewards[1])\n",
    "            print(tf.reduce_max(future_rewards, axis=1)[1])\n",
    "            print(updated_q_values[1])\n",
    "            print(target_values[1])\n",
    "        #update model\n",
    "        self.model.fit(state_batch, target_values, \n",
    "                       batch_size=self.sample_batch_size, \n",
    "                       verbose=self.model_verbose)\n",
    "        \n",
    "        #update exploration rate\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            if not self.linear_decrease:\n",
    "                self.exploration_rate *= self.exploration_decay\n",
    "            else:\n",
    "                self.exploration_rate -= self.exploration_decay\n",
    "    \n",
    "    def build_name(self, name=None):\n",
    "        if name != None:\n",
    "            name = name + \"-\"\n",
    "        else:\n",
    "            name = \"\"\n",
    "        now = datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        return f\"{self.default_name}-{name}{now}\"\n",
    "        \n",
    "    def save_model(self, name=\"\", save_memory=False):\n",
    "        self.model.save(f\"./models/{self.build_name(name=name)}.h5\")\n",
    "        if save_memory:\n",
    "            pass\n",
    "            #pickle.dump(self.memory, open(f\"./models/{self.build_name(name=name)}.pkl\", \"wb\"))\n",
    "        print(f\"MODEL SAVED AS ./models/{self.build_name(name=name)}.h5\")\n",
    "    \n",
    "    \n",
    "    def load_model(self, overwrite_epsilon=-1):\n",
    "        #set an value for epsilon to overwrite it\n",
    "        if overwrite_epsilon == -1:\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        else:\n",
    "            self.exploration_rate = overwrite_epsilon\n",
    "            \n",
    "        name = input(\"model file (in ./models):\")\n",
    "        self.model = keras.models.load_model(f\"./models/{name}.h5\")\n",
    "        self.target_model = self.model\n",
    "        pkl_name = input(\"replay memory file (enter x if none): \")\n",
    "        \n",
    "        if pkl_name == \"x\":\n",
    "            pass\n",
    "        else:\n",
    "            pass#self.memory = pickle.load(open(f\"./models/{pkl_name}.pkl\", \"rb\"))\n",
    "        print(f\"LOADED ./models/{name}.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a867ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Adam: lr=0.001\n",
      "Model: \"live_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_53 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 24)                48        \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"target_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_54 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 24)                48        \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb583f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    NAME: DeepQNetwork\n",
      "                    INPUT SHAPE: 1, OUTPUT SHAPE: 1\n",
      "                    OPTIMIZER: <class 'keras.optimizers.optimizer_v2.adam.Adam'>\n",
      "                    LOSS FUNCTION: <class 'keras.losses.MeanSquaredError'>\n",
      "                    LEARNING RATE: 0.001\n",
      "                    TARGET_NETWORK: enabled-> True\n",
      "                                    target_updates->0\n",
      "                    MEMORY: 0/1000000 ¦ 0.00%\n",
      "                    EPSILON: 1.000000\n",
      "                             min-> 0.0001\n",
      "                    ACTIONS TAKEN:  greedy-> 0\n",
      "                                    exploration-> 0\n",
      "                    REPLAY: batch_size-> 32\n",
      "                            gamma-> 0.95\n",
      "                            gradient_updates-> 0\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741281d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad5417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
