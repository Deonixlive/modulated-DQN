{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb \n",
    "from DQN.modulatedDQN import Agent\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import losses\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5472f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#agent and environment optimized for Atari games\n",
    "#https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "class AtariGame:\n",
    "    def __init__(self, env_name, render_mode=None, name=None):\n",
    "        self.name = name\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes          = 5e6\n",
    "        self.env_name          = env_name\n",
    "        self.render_mode       = render_mode\n",
    "        self.env               = wrap_deepmind(env = make_atari(self.env_name, render_mode=render_mode),\n",
    "                                               frame_stack=True, scale=True)\n",
    "        self.frame_skip        = 4 #skip every 4th frame\n",
    "        #84x84 greyscale\n",
    "        self.reduzed_size      = (84, 84, self.frame_skip) #, greyscale(1), 84x84, 4 Pictures, \n",
    "        self.state_size        = self.reduzed_size\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        self.termination_index = 10000\n",
    "        self.history           = []\n",
    "        self.save_freq         = 10000\n",
    "        self.update_freq       = 10000 #update target every 10k frames\n",
    "        self.random_min        = 50000\n",
    "        self.agent             = Agent(self.state_size, self.action_size, \n",
    "                                       name=f\"{self.env_name}-modulatedDQN\",\n",
    "                                       linear_decrease=True,\n",
    "                                       epsilon=1,\n",
    "                                       epsilon_decay=0.9/31000/4,\n",
    "                                       epsilon_min=0.01,\n",
    "                                       model_verbose=0,\n",
    "                                       lr=6.25e-5,\n",
    "                                       gamma=0.99,\n",
    "                                       optimizer=keras.optimizers.Adam(learning_rate=1.5e-4, clipnorm=1.0),\n",
    "                                       loss_function=losses.Huber(),\n",
    "                                       #pass integers only\n",
    "                                       max_memory_size=int(3.5e5),\n",
    "                                       compress_memory=True\n",
    "                                       )\n",
    "        #print(self.state_size, self.action_size)\n",
    "    \n",
    "    def run(self, load_model = False, skip_training=False, overwrite_epsilon=-1, save=True, logs=False, log_freq=1):\n",
    "        #LOGS ARE ALWAYS ENABLED\n",
    "        logging.basicConfig(filename=f\"models/{self.agent.default_name}.log\",\n",
    "                level=logging.INFO,\n",
    "                format='%(levelname)s: %(asctime)s %(message)s',\n",
    "                datefmt='%d/%m/%Y %I:%M:%S')\n",
    "        \n",
    "        #if true, try to load existing model\n",
    "        if load_model:\n",
    "            self.agent.load_model(overwrite_epsilon=overwrite_epsilon)\n",
    "            \n",
    "        try:\n",
    "            training_batches = 0\n",
    "            frames = 0\n",
    "            for index_episode in range(int(self.episodes)):\n",
    "                state = self.env.reset() #returns a LazyFrame\n",
    "                state = np.array(state)\n",
    "                state_tensor = tf.expand_dims(state, 0)\n",
    "                done = False\n",
    "                score = 0 \n",
    "                q = 0\n",
    "                q_n = 0\n",
    "                \n",
    "                for index in range(0, self.termination_index):\n",
    "                    if index > self.frame_skip or frames > self.random_min:\n",
    "                        action = self.agent.pick_action(state_tensor) \n",
    "                    else:\n",
    "                        action = np.random.choice(self.agent.action_size)\n",
    "                     \n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    next_state = np.array(next_state)\n",
    "                    next_state_tensor = tf.expand_dims(next_state, 0)     \n",
    "                    score += reward\n",
    "                    #print(next_state.numpy().shape)\n",
    "                    #store Lazyframes\n",
    "                    self.agent.update_memory(state=state, \n",
    "                                             reward=reward, \n",
    "                                             action=action, \n",
    "                                             state_next=next_state, \n",
    "                                             done=done)\n",
    "                    \n",
    "                    if (index % self.frame_skip == 0) and not skip_training:\n",
    "                        if index > self.frame_skip:\n",
    "                            q += np.amax(self.agent.predict(state_tensor))\n",
    "                            q_n += 1\n",
    "\n",
    "                            training_batches += 1\n",
    "                            self.agent.replay(debug=False)  \n",
    "                            \n",
    "                    if (frames % self.update_freq) == 0:\n",
    "                        self.agent.update_target()\n",
    "                        logging.info(f\"MODEL UPDATE\")\n",
    "                        \n",
    "                    state = next_state\n",
    "                    state_tensor = next_state_tensor\n",
    "                    \n",
    "                    frames += 1\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                self.history.append(score)\n",
    "                if len(self.history) > 100:\n",
    "                    del self.history[:1]\n",
    "                \n",
    "                print(f\"Episode: {index_episode:-10}\")\n",
    "                print(f\"Score: {score:-12}\")\n",
    "                print(f\"Epsilon: {self.agent.exploration_rate}\")\n",
    "                print(\"\".join([\"_\" for i in range(10)]))\n",
    "                if logs and index_episode % log_freq == 0:\n",
    "                    running_reward = sum(self.history) / len(self.history)\n",
    "                    logging.info(f\"EPISODE: {index_episode}\")\n",
    "                    logging.info(f\"AVG LAST {len(self.history)} REWARDS: {running_reward:0.2f}\")\n",
    "                    \n",
    "                    if q_n != 0:\n",
    "                        logging.info(f\"AVG Q VALUE: {(q / q_n):.5f}\")\n",
    "                    logging.info(f\"SCORE: {score}\")\n",
    "                    logging.info(f\"DURATION (STEPS): {index}\")\n",
    "                    logging.info(str(self.agent))\n",
    "                    logging.info(\"\".join([\"-\" for i in range(12)]))\n",
    "                \n",
    "                if index_episode % self.save_freq == 0 and save and index_episode != 0:\n",
    "                    self.agent.save_model(name=\"EP\"+str(index_episode/1000)+\"k\", save_memory=False)\n",
    "                \n",
    "        except:\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"ERROR\", save_memory=False)\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            logging.exception(\"An error has occured\")\n",
    "            logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            \n",
    "        finally:\n",
    "            #save model upon interrupting\n",
    "            if logs or save:\n",
    "                logging.info(f\"TRAINING FINISHED AFTER {training_batches} BATCHES\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "                if save:\n",
    "                    model_name = self.agent.build_name(name=\"FINISHED\")\n",
    "                    logging.info(f\"MODEL NAME: {model_name}\")\n",
    "                logging.info(\"\".join([\"+\" for i in range(14)]))\n",
    "            #print(training_batches)\n",
    "            if save:\n",
    "                self.agent.save_model(name=\"FINISHED\", save_memory=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #render modes: \"human\", None\n",
    "    atari = AtariGame(\"BreakoutNoFrameskip-v4\", render_mode=None)\n",
    "    atari.run(load_model=False, skip_training=False, overwrite_epsilon=1, save=False, logs=True, log_freq=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
    "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "env1 = make_atari(\"BreakoutNoFrameskip-v4\")\n",
    "env.seed(1)\n",
    "env1.seed(1)\n",
    "agent = Agent(env.observation_space.shape, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.array(state)\n",
    "print(state.dtype)\n",
    "plt.imshow(state)\n",
    "plt.savefig(fname=\"Breakout_pre\",dpi=600)\n",
    "\n",
    "compressed = agent.replay_compress(state)\n",
    "uncompressed = agent.replay_unpack(compressed)\n",
    "\n",
    "np.equal(state, uncompressed).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = env1.reset()\n",
    "\n",
    "state1 = env1.step()\n",
    "plt.imshow(state1)\n",
    "# plt.savefig(fname=\"Breakout_raw\",dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1d404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9b7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830fc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
