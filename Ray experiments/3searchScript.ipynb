{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import ray.rllib as rllib\n",
    "from ray import tune\n",
    "import ray\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import gym\n",
    "\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from ray.rllib.utils.exploration.epsilon_greedy import EpsilonGreedy\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.typing import ModelConfigDict, TensorType\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "\n",
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.dqn import DQN \n",
    "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel    \n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "\n",
    "# from ray.rllib.env.wrappers.atari_wrappers import \n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c69757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ray-project/ray/blob/master/rllib/examples/custom_keras_model.py\n",
    "# Eigendefiniertes Q_Model\n",
    "# class AtariModel(DistributionalQTFModel):\n",
    "class DualAtariModel(DistributionalQTFModel):\n",
    "    def __init__(self,\n",
    "                obs_space,\n",
    "                action_space,\n",
    "                num_outputs,\n",
    "                model_config,\n",
    "                name,\n",
    "                **kw):\n",
    "        \n",
    "        super(DualAtariModel, self).__init__(\n",
    "            obs_space, \n",
    "            action_space, \n",
    "            num_outputs, \n",
    "            model_config, \n",
    "            name, \n",
    "            **kw\n",
    "        )\n",
    "#         print(obs_space)\n",
    "        self.inputs = tf.keras.layers.Input(shape=obs_space.shape,\n",
    "                                           name=\"input_layer\")\n",
    "        layer1 = tf.keras.layers.Conv2D(32, 8, strides=4,\n",
    "                                       name=\"layer1\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(self.inputs)\n",
    "        layer2 = tf.keras.layers.Conv2D(64, 4, strides=2,\n",
    "                                       name=\"layer2\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(layer1)\n",
    "        layer3 = tf.keras.layers.Conv2D(64, 3, strides=1,\n",
    "                                       name=\"layer3\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(layer2)\n",
    "        flatten = tf.keras.layers.Flatten()(layer3)\n",
    "        \n",
    "        dense = tf.keras.layers.Dense(num_outputs,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name=\"dense\",\n",
    "                                        kernel_initializer=normc_initializer(1.0)\n",
    "                                        )(flatten)\n",
    "        \n",
    "        #doesnt output the actual q-values. this is handled in the q-head\n",
    "        self.base_model = tf.keras.Model(self.inputs, dense)\n",
    "\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "#         print(input_dict[\"obs\"])\n",
    "        model_out = self.base_model(input_dict[\"obs\"])\n",
    "        return model_out, state\n",
    "\n",
    "ModelCatalog.register_custom_model(\"DualAtariModel\", DualAtariModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3681ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generelle Einstellungen\n",
    "config = DQNConfig().to_dict()\n",
    "\n",
    "DQNConfigTrainer = {\n",
    "    #DQN\n",
    "    \"dueling\": True,\n",
    "    \"double_q\": True,\n",
    "    \"target_network_update_freq\": 8000,\n",
    "    \"num_gpus\": 0.2,\n",
    "#     \"num_workers\": 2,\n",
    "#     \"num_envs_per_worker\": 2,\n",
    "    \"rollout_fragment_length\": 4, #4 = num_envs_per_worker * rollout_fragment_length\n",
    "    #Einstellungen f√ºr den Algorithmus\n",
    "    \"env\": tune.grid_search([\"BreakoutNoFrameskip-v4\",\"SpaceInvadersNoFrameskip-v4\", \"PongNoFrameskip-v4\", \"StarGunnerNoFrameskip-v4\"]),\n",
    "#     \"env\": \"SpaceInvadersNoFrameskip-v4\",\n",
    "#     \"env_config\" : {\n",
    "#         \"name\": \"BreakoutNoFrameskip-v4\"\n",
    "#     },\n",
    "    \"noisy\": False,\n",
    "    \"num_atoms\": 1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 0.0000625, #Lernrate des TD-Errors\n",
    "    \"adam_epsilon\": 0.00015,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"hiddens\": [512],\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"DualAtariModel\",\n",
    "        #architecture of advantage, value streams\n",
    "#         \"custom_model_config\": {\"hiddens\": [512]}\n",
    "    },\n",
    "#     \"clip_rewards\": True, #Clippen zu -1, 0 und 1\n",
    "#     \"preprocessor_pref\": \"deepmind\",\n",
    "    #Exploration\n",
    "    \"explore\": True,\n",
    "    \"exploration_config\": {\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        \"epsilon_timesteps\": 200000,\n",
    "        \"final_epsilon\": 0.01\n",
    "    },\n",
    "    \"evaluation_interval\": int(5e4),\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \n",
    "    },\n",
    "    #Kompression\n",
    "    \"compress_observations\": True,\n",
    "    #Replay Buffer\n",
    "    \"replay_buffer_config\": {\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "#             \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer. Note that if async_updates is set,\n",
    "            # then each worker will have a replay buffer of this size.\n",
    "            \"capacity\": int(1e6),\n",
    "            \"prioritized_replay_alpha\": 0.7,\n",
    "            # Beta parameter for sampling from prioritized replay buffer.\n",
    "            \"prioritized_replay_beta\": 0.5,\n",
    "            # Epsilon to add to the TD errors when updating priorities.\n",
    "            \"prioritized_replay_eps\": 1e-6,\n",
    "            # The number of continuous environment steps to replay at once. This may\n",
    "            # be set to greater than 1 to support recurrent models.\n",
    "            \"replay_sequence_length\": 1,\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        },\n",
    "}\n",
    "\n",
    "config.update(DQNConfigTrainer)\n",
    "\n",
    "#TEST\n",
    "# del config[\"model\"]\n",
    "# config[\"hiddens\"] = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309319f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\"DQN\",\n",
    "                    name=\"Test\",\n",
    "                    stop={\"agent_timesteps_total\": int(2e7)},\n",
    "                    config=config,\n",
    "                    keep_checkpoints_num=60,\n",
    "                    checkpoint_freq=int(1e6),\n",
    "                    checkpoint_at_end=True,\n",
    "                    num_samples=1,\n",
    "                    local_dir=\"/users/stud/c/chau0000/Scripts/results\"\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
