{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293a0d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n",
      "/home/deonix/miniconda3/envs/rllib/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib as rllib\n",
    "from ray import tune\n",
    "import ray\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import gym\n",
    "\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from ray.rllib.utils.exploration.epsilon_greedy import EpsilonGreedy\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "\n",
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.dqn import DQN \n",
    "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
    "\n",
    "\n",
    "# from ray.rllib.env.wrappers.atari_wrappers import \n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8770a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atari_env(env_config: dict):\n",
    "#     render_mode = env_config.get(\"render_mode\", None)\n",
    "#     env = gym.make(env_config[\"name\"], render_mode=render_mode)\n",
    "    \n",
    "# #     env = wrap_deepmind(env, frame_stack=True, scale=False)\n",
    "#     return env\n",
    "\n",
    "# register_env(\"atari_env\", atari_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c69757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ray-project/ray/blob/master/rllib/examples/custom_keras_model.py\n",
    "# Eigendefiniertes Q_Model\n",
    "class AtariModel(DistributionalQTFModel):\n",
    "    def __init__(self,\n",
    "                obs_space,\n",
    "                action_space,\n",
    "                num_outputs,\n",
    "                model_config,\n",
    "                name,\n",
    "                **kw):\n",
    "        \n",
    "        super(AtariModel, self).__init__(\n",
    "            obs_space, \n",
    "            action_space, \n",
    "            num_outputs, \n",
    "            model_config, \n",
    "            name, \n",
    "            **kw\n",
    "        )\n",
    "#         print(obs_space)\n",
    "        self.inputs = tf.keras.layers.Input(shape=obs_space.shape,\n",
    "                                           name=\"input_layer\")\n",
    "        layer1 = tf.keras.layers.Conv2D(32, 8, strides=4,\n",
    "                                       name=\"layer1\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(self.inputs)\n",
    "        layer2 = tf.keras.layers.Conv2D(64, 4, strides=2,\n",
    "                                       name=\"layer2\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(layer1)\n",
    "        layer3 = tf.keras.layers.Conv2D(64, 3, strides=1,\n",
    "                                       name=\"layer3\",\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=normc_initializer(1.0)\n",
    "                                       )(layer2)\n",
    "        flatten = tf.keras.layers.Flatten()(layer3)\n",
    "#         layer_4 = tf.keras.layers.Dense(\n",
    "#                                         512,\n",
    "#                                         activation=tf.nn.relu,\n",
    "#                                         name=\"dense1\",\n",
    "#                                         kernel_initializer=normc_initializer(1.0)\n",
    "#                                         )(flatten)\n",
    "        layer_out = tf.keras.layers.Dense(\n",
    "                                        num_outputs,\n",
    "                                        name=\"my_out\",\n",
    "                                        activation=None,\n",
    "                                        kernel_initializer=normc_initializer(0.01),\n",
    "                                        )(flatten)\n",
    "\n",
    "        self.base_model = tf.keras.Model(self.inputs, layer_out)\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.base_model(input_dict[\"obs\"])\n",
    "        \n",
    "        return model_out, state\n",
    "    \n",
    "#     def metrics(self):\n",
    "#         return {\"foo\": tf.constant(42.0)}\n",
    "\n",
    "\n",
    "ModelCatalog.register_custom_model(\"AtariModelv2\", AtariModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3681ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generelle Einstellungen\n",
    "config = DQNConfig().to_dict()\n",
    "\n",
    "DQNConfigTrainer = {\n",
    "    #DQN\n",
    "    \"dueling\": True,\n",
    "    \"double_q\": True,\n",
    "    \"target_network_update_freq\": 8000,\n",
    "    \"num_gpus\": 1,\n",
    "#     \"num_workers\": 3,\n",
    "#     \"num_envs_per_worker\": 2,\n",
    "    \"rollout_fragment_length\": 4, #4 = num_envs_per_worker * rollout_fragment_length\n",
    "    #Einstellungen fÃ¼r den Algorithmus\n",
    "    \"env\": \"BreakoutNoFrameskip-v4\",\n",
    "#     \"env_config\" : {\n",
    "#         \"name\": \"BreakoutNoFrameskip-v4\"\n",
    "#     },\n",
    "    \"noisy\": False,\n",
    "    \"n_step\": 1,\n",
    "    \"num_atoms\": 1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"adam_epsilon\": 0.00015,\n",
    "    \"lr\": 0.0000625, #Lernrate des TD-Errors\n",
    "    \"train_batch_size\": 32,\n",
    "    \"hiddens\": [512],\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"AtariModelv2\",\n",
    "        \n",
    "    },\n",
    "#     \"clip_rewards\": False, #Clippen zu -1, 0 und 1\n",
    "#     \"preprocessor_pref\": \"deepmind\",\n",
    "    #Exploration\n",
    "    \"explore\": True,\n",
    "    \"exploration_config\": {\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        \"epsilon_timesteps\": 200000,\n",
    "        \"final_epsilon\": 0.01\n",
    "    },\n",
    "#     \"evaluation_interval\": int(5e4),\n",
    "#     \"evaluation_num_workers\": 1,\n",
    "#     \"evaluation_config\": {\n",
    "#         \"num_envs_per_worker\": 1,\n",
    "#         \"explore\": False,\n",
    "#         \"render_env\": True,\n",
    "#         \"env_config\": {\n",
    "#             \"name\": \"BreakoutNoFrameskip-v4\",\n",
    "#             \"render_mode\": \"rgb_array\",\n",
    "#             \"record_env\": True\n",
    "#         }\n",
    "#         \"record_env\": True\n",
    "#     },\n",
    "    #Kompression\n",
    "    \"compress_observations\": True,\n",
    "    #Replay Buffer\n",
    "    \"replay_buffer_config\": {\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            # Specify prioritized replay by supplying a buffer type that supports\n",
    "            # prioritization, for example: MultiAgentPrioritizedReplayBuffer.\n",
    "#             \"prioritized_replay\": DEPRECATED_VALUE,\n",
    "            # Size of the replay buffer. Note that if async_updates is set,\n",
    "            # then each worker will have a replay buffer of this size.\n",
    "            \"capacity\": 200000,\n",
    "            \"prioritized_replay_alpha\": 0.6,\n",
    "            # Beta parameter for sampling from prioritized replay buffer.\n",
    "            \"prioritized_replay_beta\": 0.4,\n",
    "            # Epsilon to add to the TD errors when updating priorities.\n",
    "            \"prioritized_replay_eps\": 1e-6,\n",
    "            # The number of continuous environment steps to replay at once. This may\n",
    "            # be set to greater than 1 to support recurrent models.\n",
    "            \"replay_sequence_length\": 1,\n",
    "            # Whether to compute priorities on workers.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        },\n",
    "}\n",
    "\n",
    "config.update(DQNConfigTrainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\"DQN\",\n",
    "                    name=\"BreakoutDQNruntest\",\n",
    "#                     stop={\"episode_reward_mean\": 40},\n",
    "                    config=config,\n",
    "                    checkpoint_score_attr=\"episode_reward_mean\",\n",
    "                    keep_checkpoints_num=5,\n",
    "                    checkpoint_freq=10,\n",
    "                    checkpoint_at_end=True,\n",
    "#                     resume=\"AUTO\",\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.ExperimentAnalysis( \n",
    "    experiment_checkpoint_path=\"~/ray_results/BreakoutDQNrun2/experiment_state-2022-09-06_08-34-31.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.default_mode = \"max\"\n",
    "analysis.default_metric = \"episode_reward_mean\"\n",
    "analysis.best_trial\n",
    "best_check = analysis.get_best_checkpoint(trial=analysis.get_best_trial())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f676ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQN(config=config)\n",
    "dqn_agent.restore(best_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79abf41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn_agent.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AlgorithmConfig().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c99a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.get(\"env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = atari_env({\"name\": \"BreakoutNoFrameskip-v4\", \"render_mode\": \"human\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d571fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "c = 3\n",
    "a = b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = dqn_agent.get_policy()\n",
    "model = policy.model\n",
    "model.base_model.summary() \n",
    "model.q_value_head.summary()\n",
    "model.state_value_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee47f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c200e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
